#1 Read in the data to R
dataset <- read_excel("~/Box/Teaching/Applied Data Science (IST 687)/Datasets/MedianZIP.xlsx")
#view(dataset)
dataset <- dataset[-1,]
# 2 remove any info at the front of the file that's not needed. THis will require you to explore the data FYI. You'll need to remove the commas for numbers and convert to numeric (function hints: gsub() and as.numeric() ). Change column names to "zip", "Median","Mean", and "Population
#colnames(dataset)
colnames(dataset) <- c("Zip", "Median", "Mean", "Population")
#head(dataset)
#str(dataset)
dataset$Median <- as.numeric(gsub(",", "", dataset$Median))
dataset$Mean <- as.numeric(gsub(",", "", dataset$Mean))
dataset$Population <- as.numeric(gsub(",", "", dataset$Population))
#str(dataset)
# 3 Install and load the "zipcode" package. There is a dataset in zipcode that contains the following information: "zip","city","state","latitude","longitude". See NOTE below)
library('zipcode')
data(zipcode)
############### zip_df <- subset(zipcode)
# 4 Merge the zipcode dataframe information into one dataframe using the merge() function. FYI you'll need to clean the zipcodes in in the original data by running clean.zipcodes(zips). clean.zipcodes is a function in the zipcode package.
##########  dataset <- clean.zipcodes(dataset)
dataset$Zip <- clean.zipcodes(dataset$Zip) ####### COREY
merge_df <- merge(dataset, zip_df, by.x="Zip", by.y ="zip")
zip_df <- subset(zipcode)
merge_df <- merge(dataset, zipcode, by.x="Zip", by.y ="zip") # I REPLACED zip_df with zipcode. Theses are the same data
merge_df <- merge_df[merge_df$state != "HI",]
merge_df <- merge_df[merge_df$state != "AK",]
##########    merge_df <- merge_df[merge_df$state != "DC",] NOT DC
head(merge_df)
grouped_df <- sqldf('SELECT state, AVG("Median") AS Median_income, SUM("Population") AS Pop FROM
merge_df
GROUP BY
state')
View(grouped_df)
grouped_df$state_name <- state.name[match(grouped_df$state,state.abb)] #THIS CODE TAKES YOUR DATAFRAME AND MATCHES THE STATE IN THE DATAFRAME WITH THE STATE.ABB DATA FROM BASE R. RUN STATE.ABB IN THE CONSOLE AND YOU'LL SEE THE ABBREVATIONS
View(grouped_df)
grouped_df$state_name <- tolower(grouped_df$state_name)
U.S <- map_data("state")
#map.income <- ggplot(grouped_df,aes(map_id = StateName)) + geom_map(map=U.S,aes(fill=median_income)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Median Income")
# 4 Show the U.S. map, with color representing the population of the state using the function tolower()
map.pop <- ggplot(grouped_df,aes(map_id = StateName)) + geom_map(map=U.S,aes(fill=pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
map.pop
map.pop
names(grouped_df)
map.pop <- ggplot(grouped_df,aes(map_id = StateName)) + geom_map(map=U.S,aes(fill=Pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
map.pop <- ggplot(grouped_df,aes(map_id = StateName)) + geom_map(map=U.S,aes(fill=Pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
map.pop
ggplot(grouped_df,aes(map_id = state_name)) + geom_map(map=U.S,aes(fill=Pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
map.pop <- ggplot(grouped_df,aes(map_id = state_name)) + geom_map(map=us,aes(fill=Pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
U.S <- map_data("state")
ggplot(grouped_df,aes(map_id = state_name)) + geom_map(map=U.S,aes(fill=Pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
ggplot(grouped_df,aes(map_id = state_name)) + geom_map(map=U.S,aes(fill=Pop)) + expand_limits(x=U.S$long,y=U.S$lat) + coord_map() + ggtitle("U.S Map Population")
# load "gdata" package so that you can read from Excel
# install.packages("gdata")
library(gdata)
# read the data directly from the web into our own data frame "df"
df <- read.xls("http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/excel/mlr01.xls")
# assign column names to the dataset
colnames(df) <- c("fawn", "adult", "precipitation", "severity")
View(df)
df$fawn
fawn <- cat(df$fawn, collapse=", ")
cat(paste(shQuote(df$fawn, type="cmd"), collapse=", "))
paste(shQuote(df$fawn, type="cmd"), collapse=", ")
cat(shQuote(df$fawn, type="cmd", collapse=", "))
cat(shQuote(df$fawn, type="cmd"), collapse=", ")
fawn
df$fawn
fawn <- c(2.9,2.4,2.0,2.3,3.2,1.9,3.4,2.1)
cat(shQuote(df$fawn, type="cmd"), collapse=", ")
paste(shQuote(df$fawn, type="cmd"), collapse=", ")
df
adult <- df$adult
adult
adult <- c(9.2,8.7,7.2,8.5,9.6,6.8,9.7,7.9)
df$precipitation
precipitation <- c(13.2,11.5,10.8,12.3,12.6,10.6,14.1,11.2)
df$severity
severity <- c(2,3,4,2,3,5,1,3)
fawn
# Install packages if necessary
library(gdata)
install.packages("C:/Desktop/zipcode/")
library(zipcode)
install.packages("readxl")
library(readxl)
library(sqldf)
library(ggplot2)
library(ggmap)
# 1 Read in the data to R
mydata<-read_excel("C:/Users/markd/OneDrive/Desktop/Data Sc/Homeworks/HW7/MedianZIP.xlsx")
sbaLocation <- URLencode("http://www.historyplace.com/speeches/anthony.htm"); doc.html <- htmlTreeParse(sbaLocation, useInternal = TRUE)
setwd("~/Box/Website/cjacks04.github.io/687/Datasets")
sbaFile <- "sba.txt";ba <- scan(sbaFile, character(0),sep = "\n")
library(tm)
library(wordcloud)
library(ggplot2)
# read in the data using read.delim()
AFFIN_wordlist <- read.delim(url("https://cjacks04.github.io/687/Datasets/AFINN111.txt"))
# change column names to "Word" and "Score"
colnames(AFFIN_wordlist) <- c("Word","Score")
# read in text file MLK.txt
MLK_Text <- read.delim("MLKspeech.txt", header=F)
MLK_Text
# interprets each element of the "mlk" as a document and create a vector source
MLK_Text_Vector <- VectorSource(MLK_Text)
# create a Corpus, a "Bag of Words"
MLK_corpus <- Corpus(MLK_Text_Vector)
# first step transformation: make all of the letters in "words.corpus" lowercase
MLK_corpus <- tm_map(MLK_corpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removePunctuation)
# third step transformation: remove numbers in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
MLK_corpus <- tm_map(MLK_corpus, removeWords,stopwords("english"))
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(MLK_corpus)
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
# create a list of counts for each word named "wordCounts"
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts,decreasing = T)
wordCounts
# check the first ten items in "wordCounts" to see if it is built correctly
wordCounts[1:10]
# calculate the total number of words
totalWords <- sum(wordCounts)
word_frame <- data.frame(Word = names(wordCounts),freq=wordCounts)
wordcloud(names(wordCounts), wordCounts, min.freq=2, max.words=50, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# read in the data using read.delim()
AFFIN_wordlist <- read.delim(url("https://cjacks04.github.io/687/Datasets/AFINN111.txt"))
# change column names to "Word" and "Score"
colnames(AFFIN_wordlist) <- c("Word","Score")
# read in text file MLK.txt
MLK_Text <- read.delim("MLKspeech.txt", header=F)
MLK_Text
# interprets each element of the "mlk" as a document and create a vector source
MLK_Text_Vector <- VectorSource(MLK_Text)
# create a Corpus, a "Bag of Words"
MLK_corpus <- Corpus(MLK_Text_Vector)
# first step transformation: make all of the letters in "words.corpus" lowercase
MLK_corpus <- tm_map(MLK_corpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removePunctuation)
# third step transformation: remove numbers in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
MLK_corpus <- tm_map(MLK_corpus, removeWords,stopwords("english"))
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(MLK_corpus)
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
# create a list of counts for each word named "wordCounts"
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts,decreasing = T)
wordCounts
# check the first ten items in "wordCounts" to see if it is built correctly
wordCounts[1:10]
# calculate the total number of words
totalWords <- sum(wordCounts)
word_frame <- data.frame(Word = names(wordCounts),freq=wordCounts)
#wordcloud(names(wordCounts), wordCounts, min.freq=2, max.words=50, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# create a vector that contains all the words in "wordCounts"
words <- names(wordCounts)
# locate which words in mlk speech appeared in AFINN word list
# returns 0 if one "mlk" word does not appeared in AFINN list
setdiff(words, AFFIN_wordlist$Word)
# calculate the matched words counts
length(setdiff(words, AFFIN_wordlist$Word))
# create a new dataframe that contains matched words and their counts, set ordinal numbers as row names
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
mCounts
matched
matched
mCounts
words
# read in the data using read.delim()
AFFIN_wordlist <- read.delim(url("https://cjacks04.github.io/687/Datasets/AFINN111.txt"))
# change column names to "Word" and "Score"
colnames(AFFIN_wordlist) <- c("Word","Score")
MLK_Text <- read.delim("MLKspeech.txt", header=F)
View(MLK_Text)
# read in text file MLK.txt
MLK_Text <- read.delim("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=F)
MLK_Text
# interprets each element of the "mlk" as a document and create a vector source
MLK_Text_Vector <- VectorSource(MLK_Text)
# create a Corpus, a "Bag of Words"
MLK_corpus <- Corpus(MLK_Text_Vector)
# first step transformation: make all of the letters in "words.corpus" lowercase
MLK_corpus <- tm_map(MLK_corpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removePunctuation)
# third step transformation: remove numbers in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
MLK_corpus <- tm_map(MLK_corpus, removeWords,stopwords("english"))
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(MLK_corpus)
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
# create a list of counts for each word named "wordCounts"
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts,decreasing = T)
wordCounts
# check the first ten items in "wordCounts" to see if it is built correctly
wordCounts[1:10]
# calculate the total number of words
totalWords <- sum(wordCounts)
word_frame <- data.frame(Word = names(wordCounts),freq=wordCounts)
#wordcloud(names(wordCounts), wordCounts, min.freq=2, max.words=50, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# create a vector that contains all the words in "wordCounts"
words <- names(wordCounts)
# locate which words in mlk speech appeared in AFINN word list
# returns 0 if one "mlk" word does not appeared in AFINN list
setdiff(words, AFFIN_wordlist$Word)
# calculate the matched words counts
length(setdiff(words, AFFIN_wordlist$Word))
# create a new dataframe that contains matched words and their counts, set ordinal numbers as row names
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
mCounts
matched
MLK_Text <- read.delim("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=F)
MLK_Text
MLK_Text <- MLK_Text[which(MLK_Text!="")]
MLK_Text
MLK_Text[which(MLK_Text !="")]
MLK_Text <- readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=F)
MLK_Text <- readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=F)
library(tm)
library(readr)
MLK_Text <- readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=F)
MLK_Text <- readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=FALSE)
MLK_Text <- readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt")
MLK_Text
MLK_Text <- MLK_Text[which(MLK_Text !="")]
# read in text file MLK.txt
MLK_Text <- readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt")
#############################
MLK_Text <- MLK_Text[which(MLK_Text !="")]
#############################
# interprets each element of the "mlk" as a document and create a vector source
MLK_Text_Vector <- VectorSource(MLK_Text)
# create a Corpus, a "Bag of Words"
MLK_corpus <- Corpus(MLK_Text_Vector)
# first step transformation: make all of the letters in "words.corpus" lowercase
MLK_corpus <- tm_map(MLK_corpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removePunctuation)
# third step transformation: remove numbers in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
MLK_corpus <- tm_map(MLK_corpus, removeWords,stopwords("english"))
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(MLK_corpus)
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
# create a list of counts for each word named "wordCounts"
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts,decreasing = T)
wordCounts
# check the first ten items in "wordCounts" to see if it is built correctly
wordCounts[1:10]
# calculate the total number of words
totalWords <- sum(wordCounts)
word_frame <- data.frame(Word = names(wordCounts),freq=wordCounts)
#wordcloud(names(wordCounts), wordCounts, min.freq=2, max.words=50, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# create a vector that contains all the words in "wordCounts"
words <- names(wordCounts)
# locate which words in mlk speech appeared in AFINN word list
# returns 0 if one "mlk" word does not appeared in AFINN list
setdiff(words, AFFIN_wordlist$Word)
# calculate the matched words counts
length(setdiff(words, AFFIN_wordlist$Word))
# create a new dataframe that contains matched words and their counts, set ordinal numbers as row names
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
# change column names to "word" and "counts"
colnames(match)<-c("word","counts")
# join the dataframe "match" with "AFINN" by "word" column in match and "Word" column in AFINN
joined_df <- merge(match, AFFIN_wordlist, by.x = "word", by.y = "Word")
# calculate the overall score
Score <- sum(joined_df$counts * joined_df$Score)/totalWords
Score
# The overall score is 0.1343639
# create a function to calculate scores for each quater
myfunction <- function(q){
# q = 1, 2 ,3 , or 4
# interprets each element of the "mlk" as a document and create a vector source
words.vec <- VectorSource(MLK_Text)
# create a Corpus, which is a "Bag of Words"
words.corpus <- Corpus(words.vec)
# define "cutpoint_l" as the first cut points; round the number to get an interger
cutpoint_l <- round(length(words.corpus)*(q-1)/4) + 1
# define "cutpoint_r" as the second cut points; round the number to get an interger
cutpoint_r <- round(length(words.corpus)*q/4)
# create a word corpus for for each quarter (cut by cutpoints)
words.corpus <- words.corpus[cutpoint_l: cutpoint_r]
# word corpora transformation
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# create term document matrix
tdm <- TermDocumentMatrix(words.corpus)
m <- as.matrix(tdm)
# calculate a list of counts for each word
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
# calculate total words
totalWords <- sum(wordCounts)
# locate the mlk words appeared in Afinn list
words <- names(wordCounts)
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
colnames(match)<-c("word","counts")
# merge matched words with Afinn scores
mergedTable <- merge(match, AFFIN_wordlist, by.x = "word" ,by.y = "Word")
# calculate the total score
Score <- sum(mergedTable$counts * mergedTable$Score)/totalWords
# return the results
return(Score)
}
# apply function to first quarter
q_1_score <- myfunction(1)
# apply function to second quarter
q_2_score <- myfunction(2)
# apply function to third quarter
q_3_score <- myfunction(3)
# apply function to fourth quarter
q_4_score <- myfunction(4)
MLK_Text
words.vec <- VectorSource(MLK_Text)
# create a Corpus, which is a "Bag of Words"
words.corpus <- Corpus(words.vec)
# define "cutpoint_l" as the first cut points; round the number to get an interger
cutpoint_l <- round(length(words.corpus)*(q-1)/4) + 1
words.corpus
words.vec <- VectorSource(MLK_Text)
words.vec
words.corpus <- Corpus(words.vec)
words.corpus
round(length(words.corpus)*(q-1)/4) + 1
cutpoint_l <- round(length(words.corpus)*(1-1)/4) + 1
cutpoint_l
cutpoint_r <- round(length(words.corpus)*1/4)
cutpoint_r
# create a function to calculate scores for each quater
myfunction <- function(q){
# q = 1, 2 ,3 , or 4
# interprets each element of the "mlk" as a document and create a vector source
words.vec <- VectorSource(MLK_Text)
# create a Corpus, which is a "Bag of Words"
words.corpus <- Corpus(words.vec)
# define "cutpoint_l" as the first cut points; round the number to get an interger
cutpoint_l <- round(length(words.corpus)*(q-1)/4) + 1
# define "cutpoint_r" as the second cut points; round the number to get an interger
cutpoint_r <- round(length(words.corpus)*q/4)
# create a word corpus for for each quarter (cut by cutpoints)
words.corpus <- words.corpus[cutpoint_l: cutpoint_r]
# word corpora transformation
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# create term document matrix
tdm <- TermDocumentMatrix(words.corpus)
m <- as.matrix(tdm)
# calculate a list of counts for each word
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
# calculate total words
totalWords <- sum(wordCounts)
# locate the mlk words appeared in Afinn list
words <- names(wordCounts)
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
colnames(match)<-c("word","counts")
# merge matched words with Afinn scores
mergedTable <- merge(match, AFFIN_wordlist, by.x = "word" ,by.y = "Word")
# calculate the total score
Score <- sum(mergedTable$counts * mergedTable$Score)/totalWords
# return the results
return(Score)
}
# apply function to first quarter
q_1_score <- myfunction(1)
# apply function to second quarter
q_2_score <- myfunction(2)
# apply function to third quarter
q_3_score <- myfunction(3)
# apply function to fourth quarter
q_4_score <- myfunction(4)
q_1_score
q_2_score
q_3_score
q_4_score
MLK_Text <-readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt", header=F)
MLK_Text <-readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt")
MLK_Text <- MLK_Text[which(MLK_Text != 0)]
# read in text file MLK.txt
MLK_Text <-readLines("~/Box/Website/cjacks04.github.io/687/Datasets/MLKspeech.txt")
MLK_Text <- MLK_Text[which(MLK_Text != 0)]
# interprets each element of the "mlk" as a document and create a vector source
MLK_Text_Vector <- VectorSource(MLK_Text)
# create a Corpus, a "Bag of Words"
MLK_corpus <- Corpus(MLK_Text_Vector)
# first step transformation: make all of the letters in "words.corpus" lowercase
MLK_corpus <- tm_map(MLK_corpus, content_transformer(tolower))
# second step transformation: remove the punctuation in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removePunctuation)
# third step transformation: remove numbers in "words.corpus"
MLK_corpus <- tm_map(MLK_corpus, removeNumbers)
# final step transformation: take out the "stop" words, such as "the", "a" and "at"
MLK_corpus <- tm_map(MLK_corpus, removeWords,stopwords("english"))
# create a term-document matrix "tdm"
tdm <- TermDocumentMatrix(MLK_corpus)
# convert tdm into a matrix called "m"
m <- as.matrix(tdm)
# create a list of counts for each word named "wordCounts"
wordCounts <- rowSums(m)
# sort words in "wordCounts" by frequency
wordCounts <- sort(wordCounts,decreasing = T)
wordCounts
# check the first ten items in "wordCounts" to see if it is built correctly
wordCounts[1:10]
# calculate the total number of words
totalWords <- sum(wordCounts)
word_frame <- data.frame(Word = names(wordCounts),freq=wordCounts)
wordcloud(names(wordCounts), wordCounts, min.freq=2, max.words=50, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# create a vector that contains all the words in "wordCounts"
words <- names(wordCounts)
# locate which words in mlk speech appeared in AFINN word list
# returns 0 if one "mlk" word does not appeared in AFINN list
setdiff(words, AFFIN_wordlist$Word)
# calculate the matched words counts
length(setdiff(words, AFFIN_wordlist$Word))
# create a new dataframe that contains matched words and their counts, set ordinal numbers as row names
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
# change column names to "word" and "counts"
colnames(match)<-c("word","counts")
# join the dataframe "match" with "AFINN" by "word" column in match and "Word" column in AFINN
joined_df <- merge(match, AFFIN_wordlist, by.x = "word", by.y = "Word")
# calculate the overall score
Score <- sum(joined_df$counts * joined_df$Score)/totalWords
Score
# The overall score is 0.1343639
# create a function to calculate scores for each quater
myfunction <- function(q){
# q = 1, 2 ,3 , or 4
# interprets each element of the "mlk" as a document and create a vector source
words.vec <- VectorSource(MLK_Text)
# create a Corpus, which is a "Bag of Words"
words.corpus <- Corpus(words.vec)
# define "cutpoint_l" as the first cut points; round the number to get an interger
cutpoint_l <- round(length(words.corpus)*(q-1)/4) + 1
# define "cutpoint_r" as the second cut points; round the number to get an interger
cutpoint_r <- round(length(words.corpus)*q/4)
# create a word corpus for for each quarter (cut by cutpoints)
words.corpus <- words.corpus[cutpoint_l: cutpoint_r]
# word corpora transformation
words.corpus <- tm_map(words.corpus, content_transformer(tolower))
words.corpus <- tm_map(words.corpus, removePunctuation)
words.corpus <- tm_map(words.corpus, removeNumbers)
words.corpus <- tm_map(words.corpus, removeWords, stopwords("english"))
# create term document matrix
tdm <- TermDocumentMatrix(words.corpus)
m <- as.matrix(tdm)
# calculate a list of counts for each word
wordCounts <- rowSums(m)
wordCounts <- sort(wordCounts, decreasing=TRUE)
# calculate total words
totalWords <- sum(wordCounts)
# locate the mlk words appeared in Afinn list
words <- names(wordCounts)
matched <- match(words, AFFIN_wordlist$Word, nomatch = 0)
mCounts <- wordCounts[which(matched != 0)]
match <- data.frame(names(mCounts),mCounts,row.names = c(1:length(mCounts)))
colnames(match)<-c("word","counts")
# merge matched words with Afinn scores
mergedTable <- merge(match, AFFIN_wordlist, by.x = "word" ,by.y = "Word")
# calculate the total score
Score <- sum(mergedTable$counts * mergedTable$Score)/totalWords
# return the results
return(Score)
}
# apply function to first quarter
q_1_score <- myfunction(1)
# apply function to second quarter
q_2_score <- myfunction(2)
# apply function to third quarter
q_3_score <- myfunction(3)
# apply function to fourth quarter
q_4_score <- myfunction(4)
# combine scores of four quarters into one dataframe
quarters = c("Q1","Q2","Q3","Q4")
scores <- c(q_1_score,q_2_score,q_3_score,q_4_score)
score_df <- data.frame(scores,scores)
# create a bar plot for the four scores
ggplot(score_df, aes(x=quarters,y=scores)) + geom_bar(stat = "identity")
